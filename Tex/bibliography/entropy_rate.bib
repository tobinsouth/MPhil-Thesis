

@article{bagrowInformationFlowReveals2019,
  title = {Information flow reveals prediction limits in online social activity},
  author = {Bagrow, James P. and Liu, Xipei and Mitchell, Lewis},
  year = {2019},
  month = feb,
  volume = {3},
  pages = {122--128},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0510-5},
  journal = {Nature Human Behaviour},
  number = {2}
}


@article{shannon_prediction_1951,
  title = {Prediction and Entropy of Printed {{English}}},
  author = {Shannon, Claude E.},
  year = {1951},
  volume = {30},
  pages = {50--64},
  publisher = {{Wiley Online Library}},
  file = {/Users/tobinsouth/Zotero/storage/SZ8HXMVT/j.1538-7305.1951.tb01366.html},
  journal = {Bell system technical journal},
  number = {1}
}




@article{cover_convergent_1978,
  title = {A Convergent Gambling Estimate of the Entropy of {{English}}},
  author = {Cover, Thomas and King, Roger},
  year = {1978},
  volume = {24},
  pages = {413--421},
  publisher = {{IEEE}},
  file = {/Users/tobinsouth/Zotero/storage/EYHHYGQE/Cover and King - 1978 - A convergent gambling estimate of the entropy of E.pdf;/Users/tobinsouth/Zotero/storage/KIW2NBQV/1055912.html},
  journal = {IEEE Transactions on Information Theory},
  number = {4}
}



@article{brown_estimate_1992,
  title = {An Estimate of an Upper Bound for the Entropy of {{English}}},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Lai, Jennifer C. and Mercer, Robert L.},
  year = {1992},
  volume = {18},
  pages = {31--40},
  file = {/Users/tobinsouth/Zotero/storage/IY4QQDHJ/Brown et al. - 1992 - An estimate of an upper bound for the entropy of E.pdf;/Users/tobinsouth/Zotero/storage/HNQ7WN8H/146680.html},
  journal = {Computational Linguistics},
  number = {1}
}



@article{kontoyiannis_nonparametric_1998,
  title = {Nonparametric Entropy Estimation for Stationary Processes and Random Fields, with Applications to {{English}} Text},
  author = {Kontoyiannis, I. and Algoet, P.H. and Suhov, Yu.M. and Wyner, A.J.},
  year = {1998},
  month = may,
  volume = {44},
  pages = {1319--1327},
  issn = {00189448},
  doi = {10.1109/18.669425},
  abstract = {We discuss a family of estimators for the entropy rate of a stationary ergodic process and prove their pointwise and mean consistency under a Doeblin-type mixing condition. The estimators are Cesa`ro averages of longest match-lengths, and their consistency follows from a generalized ergodic theorem due to Maker. We provide examples of their performance on English text, and we generalize our results to countable alphabet processes and to random fields.},
  file = {/Users/tobinsouth/Zotero/storage/AXL56Z2I/Kontoyiannis et al. - 1998 - Nonparametric entropy estimation for stationary pr.pdf},
  journal = {IEEE Transactions on Information Theory},
  language = {en},
  number = {3}
}



@article{ziv_universal_1977,
  title = {A Universal Algorithm for Sequential Data Compression},
  author = {Ziv, J. and Lempel, A.},
  year = {1977},
  month = may,
  volume = {23},
  pages = {337--343},
  issn = {0018-9448},
  doi = {10.1109/TIT.1977.1055714},
  abstract = {A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.},
  file = {/Users/tobinsouth/Zotero/storage/RUBCI5PU/Ziv and Lempel - 1977 - A universal algorithm for sequential data compress.pdf},
  journal = {IEEE Transactions on Information Theory},
  language = {en},
  number = {3}
}


@article{shields_universal_1993,
  title = {Universal Redundancy Rates Do Not Exist},
  author = {Shields, Paul C.},
  year = {1993},
  volume = {39},
  pages = {520--524},
  file = {/Users/tobinsouth/Zotero/storage/3GYE2UP2/212281.html},
  journal = {IEEE transactions on information theory},
  number = {2}
}


@article{shields_universal_1995,
  title = {Universal Redundancy Rates for the Class of {{B}}-Processes Do Not Exist},
  author = {Shields, Paul and Weiss, Benjamin},
  year = {1995},
  volume = {41},
  pages = {508--512},
  file = {/Users/tobinsouth/Zotero/storage/R4CWNKAB/370156.html},
  journal = {IEEE transactions on information theory},
  number = {2}
}


@article{grassberger_estimating_1989,
  title = {Estimating the Information Content of Symbol Sequences and Efficient Codes},
  author = {Grassberger, P.},
  year = {1989},
  month = may,
  volume = {35},
  pages = {669--675},
  issn = {00189448},
  doi = {10.1109/18.30993},
  abstract = {Several variants of an algorithm for estimating Shannon entropies of symbol sequences are presented. They are all related to the Lempel-Ziv algorithm and to recent algorithms for estimating Hausdorff dimensions. The average storage and running times increase as N and N log N , respectively, with the sequence length N . These algorithms proceed basically by constructing efficient codes. They seem to be the optimal algorithms for sequences with strong long-range correlations, e.g., natural languages. An application to written English illustrates their use.},
  file = {/Users/tobinsouth/Zotero/storage/WIPB22SR/Grassberger - 1989 - Estimating the information content of symbol seque.pdf},
  journal = {IEEE Transactions on Information Theory},
  language = {en},
  number = {3}
}


@article{shields_entropy_1992,
  title = {Entropy and Prefixes},
  author = {Shields, Paul C.},
  year = {1992},
  pages = {403--409},
  file = {/Users/tobinsouth/Zotero/storage/UTP2EJIJ/Shields - 1992 - Entropy and prefixes.pdf;/Users/tobinsouth/Zotero/storage/8JMXUCX5/2244563.html},
  journal = {The Annals of Probability}
}


@inproceedings{kontoyiannis_prefixes_1994,
  title = {Prefixes and the Entropy Rate for Long-Range Sources},
  booktitle = {{{IEEE International Symposium}} on {{Information Theory}}},
  author = {Kontoyiannis, Ioannis and Suhov, Yurii M.},
  year = {1994},
  pages = {194--194},
  publisher = {{INSTITUTE OF ELECTRICAL ENGINEERS INC (IEEE)}},
  file = {/Users/tobinsouth/Zotero/storage/S6E3UPTE/Kontoyiannis and Suhov - 1994 - Prefixes and the entropy rate for long-range sourc.pdf}
}


@article{quas_entropy_1999,
  title = {An Entropy Estimator for a Class of Infinite Alphabet Processes},
  author = {Quas, Anthony N.},
  year = {1999},
  volume = {43},
  pages = {496--507},
  file = {/Users/tobinsouth/Zotero/storage/74SLQ4AK/Quas - 1999 - An entropy estimator for a class of infinite alpha.pdf},
  journal = {Theory of Probability \& Its Applications},
  number = {3}
}


@article{wyner_asymptotic_1989,
  title = {Some Asymptotic Properties of the Entropy of a Stationary Ergodic Data Source with Applications to Data Compression},
  author = {Wyner, A.D. and Ziv, J.},
  year = {Nov./1989},
  volume = {35},
  pages = {1250--1258},
  issn = {00189448},
  doi = {10.1109/18.45281},
  journal = {IEEE Transactions on Information Theory},
  language = {en},
  number = {6}
}


@inproceedings{chen_using_1993,
  title = {Using Difficulty of Prediction to Decrease Computation: {{Fast}} Sort, Priority Queue and Convex Hull on Entropy Bounded Inputs},
  shorttitle = {Using Difficulty of Prediction to Decrease Computation},
  booktitle = {Proceedings of 1993 {{IEEE}} 34th {{Annual Foundations}} of {{Computer Science}}},
  author = {Chen, Shenfeng and Reif, John H.},
  year = {1993},
  pages = {104--112},
  publisher = {{IEEE}},
  file = {/Users/tobinsouth/Zotero/storage/455NF3BM/Chen and Reif - 1993 - Using difficulty of prediction to decrease computa.pdf;/Users/tobinsouth/Zotero/storage/9TTSVMI4/366877.html}
}



@inproceedings{chen_fast_1995,
  title = {Fast Pattern Matching for Entropy Bounded Text},
  booktitle = {Proceedings {{DCC}}'95 {{Data Compression Conference}}},
  author = {Chen, Shenfeng and Reif, John H.},
  year = {1995},
  pages = {282--291},
  publisher = {{IEEE}},
  file = {/Users/tobinsouth/Zotero/storage/V6QHZ4Q9/515518.html}
}




@inproceedings{farach_entropy_1995,
  title = {On the Entropy of {{DNA}}: {{Algorithms}} and Measurements Based on Memory and Rapid Convergence},
  shorttitle = {On the Entropy of {{DNA}}},
  booktitle = {Proceedings of the Sixth Annual {{ACM}}-{{SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Farach, Martin and Noordewier, Michiel and Savari, Serap and Shepp, Larry and Wyner, Abraham and Ziv, Jacob},
  year = {1995},
  pages = {48--57},
  file = {/Users/tobinsouth/Zotero/storage/9ZJPIAFM/Farach et al. - 1995 - On the entropy of DNA Algorithms and measurements.pdf}
}


@inproceedings{juola_what_1997,
  title = {What Can We Do with Small Corpora? {{Document}} Categorization via Cross-Entropy},
  shorttitle = {What Can We Do with Small Corpora?},
  booktitle = {Proceedings of an {{Interdisciplinary Workshop}} on {{Similarity}} and {{Categorization}}, {{Department}} of {{Artificial Intelligence}}, {{University}} of {{Edinburgh}}, {{Edinburgh}}, {{UK}}},
  author = {Juola, Patrick},
  year = {1997},
  file = {/Users/tobinsouth/Zotero/storage/6JC8XNS9/Juola - 1997 - What can we do with small corpora Document catego.eps}
}



@article{shannon_mathematical_1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  volume = {27},
  pages = {379--423},
  publisher = {{Nokia Bell Labs}},
  journal = {The Bell system technical journal},
  number = {3}
}



@article{shannon_prediction_1951,
  title = {Prediction and Entropy of Printed {{English}}},
  author = {Shannon, Claude E.},
  year = {1951},
  volume = {30},
  pages = {50--64},
  publisher = {{Wiley Online Library}},
  file = {/Users/tobinsouth/Zotero/storage/SZ8HXMVT/j.1538-7305.1951.tb01366.html},
  journal = {Bell system technical journal},
  number = {1}
}


@book{cover_elements_2012,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2012},
  month = nov,
  publisher = {{John Wiley \& Sons}},
  abstract = {The latest edition of this classic is updated with new problem sets and material   The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
  isbn = {978-1-118-58577-1},
  keywords = {Computers / General,Computers / Information Technology},
  language = {en}
}


@article{debowski_is_2018,
  title = {Is {{Natural Language}} a {{Perigraphic Process}}? {{The Theorem}} about {{Facts}} and {{Words Revisited}}},
  shorttitle = {Is {{Natural Language}} a {{Perigraphic Process}}?},
  author = {D{\k{e}}bowski, {\L}ukasz},
  year = {2018},
  month = feb,
  volume = {20},
  pages = {85},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e20020085},
  abstract = {As we discuss, a stationary stochastic process is nonergodic when a random persistent topic can be detected in the infinite random text sampled from the process, whereas we call the process strongly nonergodic when an infinite sequence of independent random bits, called probabilistic facts, is needed to describe this topic completely. Replacing probabilistic facts with an algorithmically random sequence of bits, called algorithmic facts, we adapt this property back to ergodic processes. Subsequently, we call a process perigraphic if the number of algorithmic facts which can be inferred from a finite text sampled from the process grows like a power of the text length. We present a simple example of such a process. Moreover, we demonstrate an assertion which we call the theorem about facts and words. This proposition states that the number of probabilistic or algorithmic facts which can be inferred from a text drawn from a process must be roughly smaller than the number of distinct word-like strings detected in this text by means of the Prediction by Partial Matching (PPM) compression algorithm. We also observe that the number of the word-like strings for a sample of plays by Shakespeare follows an empirical stepwise power law, in a stark contrast to Markov processes. Hence, we suppose that natural language considered as a process is not only non-Markov but also perigraphic.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/tobinsouth/Zotero/storage/X828YD6C/DÄ™bowski - 2018 - Is Natural Language a Perigraphic Process The The.pdf;/Users/tobinsouth/Zotero/storage/UVR4MLEF/85.html},
  journal = {Entropy},
  keywords = {algorithmic information theory,mutual information,natural language,power laws,PPM code,stationary processes},
  language = {en},
  number = {2}
}






@article{williams_text_2015,
  title = {Text Mixing Shapes the Anatomy of Rank-Frequency Distributions},
  author = {Williams, Jake Ryland and Bagrow, James P. and Danforth, Christopher M. and Dodds, Peter Sheridan},
  year = {2015},
  month = may,
  volume = {91},
  pages = {052811},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.91.052811},
  abstract = {Natural languages are full of rules and exceptions. One of the most famous quantitative rules is Zipf's law, which states that the frequency of occurrence of a word is approximately inversely proportional to its rank. Though this ``law'' of ranks has been found to hold across disparate texts and forms of data, analyses of increasingly large corpora since the late 1990s have revealed the existence of two scaling regimes. These regimes have thus far been explained by a hypothesis suggesting a separability of languages into core and noncore lexica. Here we present and defend an alternative hypothesis that the two scaling regimes result from the act of aggregating texts. We observe that text mixing leads to an effective decay of word introduction, which we show provides accurate predictions of the location and severity of breaks in scaling. Upon examining large corpora from 10 languages in the Project Gutenberg eBooks collection, we find emphatic empirical support for the universality of our claim.},
  file = {/Users/tobinsouth/Zotero/storage/TCP6DR86/Williams et al. - 2015 - Text mixing shapes the anatomy of rank-frequency d.pdf;/Users/tobinsouth/Zotero/storage/4GPC9LL2/PhysRevE.91.html},
  journal = {Physical Review E},
  number = {5}
}



@misc{lundh_stringlib_2006,
  title = {The Stringlib library},
  author = {Lundh, Fredrik},
  year = {2006},
  month = may,
  abstract = {The stringlib library is an experimental collection of alternative string operations for Python. The fast search algorithm described below was added to Python 2.5 during the Need For Speed sprint in Reykjavik. It's currently used by the 8-bit string and Unicode implementations.},
  file = {/Users/tobinsouth/Zotero/storage/CJHVC2J2/stringlib.html},
  howpublished = {http://effbot.org/zone/stringlib.htm},
  journal = {The stringlib Library},
  type = {Web {{Article}}}
}



















