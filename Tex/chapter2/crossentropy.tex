%!TEX root = ../thesis.tex
\chapter{The one with cross entropy \label{ch:crossentropy}}

We can calculate...

\section{Entropy Rate}

Recall \autoref{def:entropyrate} of the entropy rate of a stochastic process, which while a useful theoretical tool, can be very difficult to compute. 
\todo{why is it hard to compute}

To overcome this, we seek a way to estimate the entropy of the process from a known sequence of data. In 1998 Kontoyianni et al. proved the convergence of a non-parametric entropy estimator in stationary processes~\cite{kontoyiannisNonparametricEntropyEstimation1998}.

\begin{definition}[Kontoyianni Entropy Rate] \label{def:kontoyianni}
	For a stochastic process $\mathcal{X} = \{X_i\}$, with a realisation of $n$ states and a finite alphabet,  the entropy rate can be estimated using,
		\begin{equation}\label{eq:estimate}
	H(\mathcal{X}) \approx\frac{|\mathcal{X}| \log |\mathcal{X}| }{\sum_{i=0}^n \Lambda_{i} },
	\end{equation}
	where $|\mathcal{X}|$ is the size of the alphabet and $\Lambda_{i}$ is the length of the shortest subsequence starting at position $i$ that does not appear as a contiguous subsequence in the previous $i$ symbols $X_{0}^{i}$. This can also be obtained by adding 1 to the longest match-length, 
	 \begin{equation}\label{eq:lambda}
	  \Lambda_{i}=1+\max \left\{l: X_{i}^{i+l}=X_{j}^{j+l}, 0 \leq j \leq N-i, 0 \leq l \leq N - i - j \right\}.
	 \end{equation}
\end{definition}


This idea of using matched subsequences of text draws from the original work by Lempel and Ziv~\cite{zivUniversalAlgorithmSequential1977} in compression algorithms based on coding schemes. These algorithms attempt to compress a sequence down into the smallest possible representation, which at perfect efficiency would be the entropy, $H$. However these universal coding algorithms have no universal rate of convergence \cite{shieldsUniversalRedundancyRates1993, shieldsUniversalRedundancyRates1995} and in practice other approaches are often employed, tailored to the specific application at hand.

The idea of an entropy estimator based on match lengths was originally put forward by Grassberger~\cite{grassbergerEstimatingInformationContent1989} and proved consistent for independent and identically distributed (i.i.d.) processes and mixing Markov chains~\cite{shieldsEntropyPrefixes1992}, stationary processes~\cite{kontoyiannisPrefixesEntropyRate1994} and more generally to random fields~\cite{quasEntropyEstimatorClass1999}.


Wyner and Ziv~\cite{wynerAsymptoticPropertiesEntropy1989} showed that for every ergodic process the match length $\Lambda_{n}$ grows like $\frac{\log{n}}{H}$ in probability. 

Extending from this notion Kontoyianni et al. showed the convergence of \autoref{eq:estimate} in stationary ergodic processes using the match-length $ \Lambda_{i}$. This match-length in \autoref{eq:lambda} can be seen as the length of the next phrase to be encoded in the sliding-window Lempelâ€“Ziv algorithm.


\todo{talk more about how this match length works (diagram)}

\begin{figure}[hb]
	\centering
	\begin{tikzpicture}
	% TOP ROW
	
	\node at (0,0) (a) {``{\color{blue}I do not like} green eggs and ham.};
	\node [right] at (a.east)  (b) {{\color{blue}I do not like} them, Sam-I-am''};
	
	\node [above] at (a.west) (westa){};
	\node [above] at (a.east) (easta) {};
	
	\node [above] at (b.west) (westb){};
	\node [above] at (b.east) (eastb) {};
	
	% Past brace
	\usetikzlibrary{decorations.pathreplacing}
	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=10pt} ] (westa) -- (easta)
	node[midway, below, yshift = -20pt] {Past} ;
	
	% Future
	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=10pt} ] (westb) -- (eastb)
	node[midway, below, yshift = -20pt] {Future} ;
	
	% Draw i line
	\draw (westb.north) -- ([yshift=-0.5cm]westb.south) node[below] {$i = 8$};
	
	\node [below right of = eastb] {$\Lambda_8 = 5$};
	
	% BOTTOM ROW
	
	\def\shift{0.8}
	
	
	\usetikzlibrary{positioning}
	\node [below = 1cm of a] (c) {``I do {\color{blue}not like} green eggs and ham.};
	\node [right] at (c.east)  (d) {I do {\color{blue}not like} them, Sam-I-am''};
	
	% Intermediary nodes
	\node [right = \shift cm of c.east] (shiftedceast) {};
	\node [right = \shift cm of d.west] (shifteddwest) {};
	
	\node [above] at (c.west) (westc){};
	\node [above] at (shiftedceast) (eastc) {};
	
	\node [above] at (shifteddwest) (westd){};
	\node [above] at (d.east) (eastd) {};
	
	% Past brace

	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=10pt} ] (westc) -- (eastc)
	node[midway, below, yshift = -20pt] {Past} ;
	
	% Future
	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=10pt} ] (westd) -- (eastd)
	node[midway, below, yshift = -20pt] {Future} ;
	
	% Draw i line
	\draw ([yshift=0.11cm]shifteddwest.north) -- ([yshift=-0.4cm]shifteddwest.south) node[below] {$i = 10$};
	
	\node [below right of = eastd] {$\Lambda_{10} = 3$};
	
	% Time Arrow
	\node [above = 0.4cm of westa]  (westtime) {};
	\node [above = 0.4cm of  eastb ](easttime) {};
	\draw [very thick, ->] (westtime) -- (easttime) node[midway, above] {Process} ;
	
	
	
	\end{tikzpicture}
	\caption{An example calculation of the match-length based $\Lambda_i$ applied to a line from Green Eggs and Ham by Doctor Seuss. {\color{blue} Blue text} is that which has been matched from past to the future. }
\end{figure}


Even before it's formalisation by Kontoyianni et al, similar estimators had appeared in the literature applied to experimental data to determine the entropy rates of processes~\cite{chenUsingDifficultyPrediction1993, chenFastPatternMatching1995, farachEntropyDNAAlgorithms1995, juolaWhatCanWe1997}.


% Final note
Moving forward we will assume any any discussion of the \emph{entropy rate} of a single process is assumed to be the \emph{Kontoyianni entropy rate} of that process, unless otherwise stated.


\section{Assumptions of Entropy Rate Estimation}

\todo{A discussion and plots of entropy rate convergence}


\todo{A comparison with other entropy rates}


\begin{figure}[hb]
	\centering
	\begin{tikzpicture}
	
	\node at (-1,0) {(a)};
	
	% Time Arrow
	\node at (0,0) (lefttime) {};
	\node at (10,0) (righttime) {};
	\draw [ultra thick, |->] (lefttime) -- (righttime) node[midway, above] {Process} ;
	
	\node [above of = lefttime] {2019/01/01};
	\node [above of = righttime] {2020/01/01};
	
	\foreach \x [count=\i] in {0,2.125,4.25,6.375,8.5}{
		\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=5pt,mirror,raise=10pt} ] (0.25+\x, 0) -- (1.25+\x,0)
		node[midway, below, yshift = -20pt] {$W_\i$} ;
	}


	\node at (-1,-2) {(b)};
	
	% Time Arrow
	\node at (0,-2) (lefttime) {};
	\node at (10,-2) (righttime) {};
	\draw [ultra thick, |->] (lefttime) -- (righttime) node[midway, above] {Process};
	
	\foreach \x [count=\i] in {0,2,4,6,8}{
		\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=5pt,mirror,raise=10pt} ] (0.25+\x, -2) -- (1.75+\x,-2)
		node[midway, below, yshift = -20pt] {$W_\i$} ;
	}

	\node at (-1,-4) {(c)};
	
	% Time Arrow
	\node at (0,-4) (lefttime) {};
	\node at (10,-4) (righttime) {};
	\draw [ultra thick, |->] (lefttime) -- (righttime) node[midway, above] {Process};
	
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=10 pt} ] (0.25, -4) -- (3.75,-4)
%	node[midway, below, yshift = -20 pt] {$W_1$} ;
%	
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=40 pt} ] (1.75, -4) -- (5.25,-4)
%	node[midway, below, yshift = -50 pt] {$W_2$} ;
%	
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=70 pt} ] (3.25, -4) -- (6.75,-4)
%	node[midway, below, yshift = -80 pt] {$W_3$} ;
%	
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=10 pt} ] (4.75, -4) -- (8.25,-4)
%	node[midway, below, yshift = -20 pt] {$W_4$} ;
%	
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=20pt,mirror,raise=40 pt} ] (6.25, -4) -- (9.75,-4)	node[midway, below, yshift = -50 pt] {$W_5$} ;
	
	\foreach \x/\i in {0/1,3/3,6/5}{
	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=25pt,mirror,raise=7pt} ] (0.25+\x, -4) -- (3.5+\x,-4)
	node[midway, below, yshift = -40pt] {$W_\i$} ;
}

	\foreach \x/\i  in {1.5/2,4.5/4}{
	\draw [very thick,  -, draw=gray, decorate, decoration={brace,amplitude=25pt,mirror,raise=9pt} ] (0.25+\x, -4) -- (3.5+\x,-4)
	node[gray, midway, below, yshift = -40pt] {$W_\i$} ;
}

	\end{tikzpicture}
	\caption{Demonstration on window position in within data. Windows are equispaced along the entire length of data and initially non-overlapping. As the window size increases, shown in (a) $\rightarrow$ (c), the windows remain equispaced but will become overlapping. Window sizes increase until each window is almost entirely overlapping and encompasses the majority of the process. }
\end{figure}


\section{Cross Entropy Rate}

Similar to the extension of entropy to cross entropy in \autoref{def:crossentropy}, we can generalise our notion of Kontoyianni entropy rate in \autoref{def:kontoyianni} to a \emph{cross} entropy rate which we will call the Kontoyianni cross entropy rate.

\begin{definition}[Kontoyianni Full Cross Entropy Rate]
	The cross entropy rate of a {\color{target} target process} $\mathcal{T}$ coded from a {\color{source} source process} $\mathcal{S}$ can be estimated via,
	\begin{equation}
	\hat{H}(\mathcal{T} || \mathcal{S})=\frac{N_{\mathcal{T}} \log _{2} N_{\mathcal{S}}}{\sum_{i=1}^{N_{\mathcal{T}}} \Lambda_{i}(\mathcal{T}| \mathcal{S})}
	\end{equation}
	Where $\Lambda_{i}(\mathcal{T}| \mathcal{S})$ is given by the shortest subsequence starting at position $i$ in {\color{target}target} $\mathcal{T}$ that does not appear as a contiguous subsequence in the {\color{source}source} $\mathcal{S}$.
	\begin{equation}
	\Lambda_{i}(\mathcal{T}| \mathcal{S}) = \max \left\{l: T_i^{i+l}=S_{j}^{j+l}, 0 \leq j \leq N_{\mathcal{S}},  0 \leq l \leq \min( N_{\mathcal{S}}- j , N_{\mathcal{T}}- i )\right\},
	\end{equation}
	where $T_a^{b}$ and $S_a^b$ are continuous subsequences starting from index $a$ to index $b$ of the {\color{target} target}, $\mathcal{T}$, and  {\color{source} source}, $\mathcal{S}$, processes respectively.
\end{definition}

This approach is matching segments of text in the {\color{target}target} to segments of text anywhere in the {\color{source}source} in the same manner that the Kontoyianni entropy rate matched segments of text in the future of a process from a index to the history before the index. In essence, the entropy rate was asking how much information was needed on average to describe the future of a source from it's past. Comparatively, the full cross entropy rate above is taking asking how much information was needed on average to describe the {\color{target}target} given knowledge of the entire lifetime of the {\color{source}source}. \ts{Not sure how long I should keep up the colouring for.}

In the context of our problem, this estimator is cheating by viewing the future of news through the lens of the source process. As \todo{ref} illustrates, the text subsequence match in the target could be drawn from future time points in the source. This approach is interesting as it asks a simpler but very relevant question; how different are the two sources from an full information perspective.

\todo{tikz of how the full entropy works}

\begin{figure}
	\begin{tikzpicture}

%% BLUE EVENTS
% Main
\draw [thick] (0,0) rectangle (13,1) ;

% Events
% Past Events
\foreach \x in {0, 1, 3, 4.5, 5.5, 7.5, 8, 8.7, 9.5}
{
	\fill[blue!40!white] (\x,0) rectangle ++(0.4,1);
}
% Future Events
\foreach \x in {10.2, 11, 12.5}
{
	\fill[blue!80!white] (\x,0) rectangle ++(0.4,1);
}

% Redraw Main Rectangle
\draw [thick] (0,0) rectangle (13,1) ;


% Time Arrow
\draw [very thick, ->] (0,2) -- (13,2) node[midway, above] {Time} ;



% Text
\draw[very thick,  blue] (1.2, 0.5 ) |- (2, -1) node [right] {Tweet: ``{\color{orange} Donald Trump threatens \underline{war} on} Iran''};
\draw[very thick,  blue] (11.2, 0.5 ) |- (10, -1.7) node [left] {Tweet: ``{\color{orange}Donald Trump threatens \underline{war} on} Syria''};
\draw[very thick,  blue] (12.7, 0.5 ) |- (10.5, -2.5) node [left] {Tweet: ``World \underline{war} 3 memes are now trending''};

% Now
\draw [red!60!black, very thick, shorten >= -0.6pt]        (10,-0.2 ) -- (10,1.2) node[above] {Now};

%		% Past brace
%	\usetikzlibrary{decorations.pathreplacing}
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=4pt} ] (9.8,1) -- (0.2,1)
%	node[midway, above, yshift = 14pt] {Past} ;
\end{tikzpicture}
\end{figure}


While this question is interesting, and will be explored further, we need to refine this definition to help answer our question of information flow.

Rather than looking at the entire lifetime of the source during a matching calculations, we can reduce our search space to the text that occurred in the \emph{past} of the source. To achieve this we use an important piece of our data, the time that tweets occurred. For each word in the target process, $T_i$ has an associated time with it, $t(T_i)$. When matching the future of $\mathcal{T}$, starting from an index $i$, we can reduce the source process, $\mathcal{S}$ to only the words that were themselves tweeted before time $t(T_i)$. 

Put simply, we can alter the Kontoyianni full cross entropy rate to a time-synced cross entropy rate by replace the full {\color{source}} process, $\mathcal{S}$, with a time reduce source process $\mathcal{S}_{ \leq t(T_i)}$. This can be seen visually in \todo{ref tikz} and is formally defined as follows.

\begin{definition}[Kontoyianni Time-synced Cross Entropy Rate]
	The time-synced cross entropy rate of a {\color{target} target process} $\mathcal{T}$ coded from a {\color{source} source process} $\mathcal{S}$ can be estimated via,
	\begin{equation}
	\hat{H}(\mathcal{T} || \mathcal{S})=\frac{N_{\mathcal{T}} \log _{2} N_{\mathcal{S}}}{\sum_{i=1}^{N_{\mathcal{T}}} \Lambda_{i}(\mathcal{T}| \mathcal{S}_{\leq t(T_i) } )}
	\end{equation}
	Where $\Lambda_{i}(\mathcal{T}| \mathcal{S}_{\leq t(T_i) })$ is given by the shortest subsequence starting at position $i$ in {\color{target}target} $\mathcal{T}$ that does not appear as a contiguous subsequence in the time reduced {\color{source}source} $\mathcal{S}_{\leq t(T_i) }$ where,
	\begin{equation}
	\mathcal{S}_{\leq t(T_i) } = \{S_j | t(S_j) \leq t(T_i) \forall i \}.
	\end{equation}
	Which gives,
	\begin{align*}
	\Lambda_{i}(\mathcal{T}| \mathcal{S}_{\leq t(T_i)}) = \max \{l: T_i^{i+l}=S_{j}^{j+l}, 0 \leq j \leq N_{\mathcal{S}},  \\  0 \leq l \leq \min( N_{\mathcal{S}}- j , N_{\mathcal{T}}- i ) \},
	\end{align*}
	where $T_a^{b}$ and $S_a^b$ are continuous subsequences starting from index $a$ to index $b$ of the {\color{target} target}, $\mathcal{T}$ process, and the time reduced {\color{source} source}, $\mathcal{S}_{\leq t(T_i)}$, respectively.
\end{definition}
 
% Informantion flow idea
This time-synced entropy rate is testing not just the differences in the language processes of the source and target, but also measuring what information in the target is present in the source's history. This is an important distinction, as it allows us to probe a very important aspect of our data, namely, the time in which news is created. 

If a piece of information appears earlier in the source than in the target, it will be detected during the match length search, resulting in a lower entropy. This is to say, in the context of news, if the {\color{source}source} breaks a story first, \emph{less} information is required to describe the subsequent news output from the {\color{target}target}. 

Conversely, if a {\color{target}target} produces a piece of information before the {\color{source}source}, then that information will not appear in the history of the time-synced source during the match-length search. This will result in lower values of $\Lambda_i$ for that piece of information, which raises the cross entropy rate.

From this, we can find that, on average, if a {\color{source}source} produces information earlier than a {\color{target}target}, the cross entropy rate, $\hat{H}(\mathcal{T} || \mathcal{S})$, will be lower than if the {\color{target}target} produces information earlier than the {\color{source}source}. This method of examining who produces information first can be extended into a notion of \emph{information flow}, a discussion we will leave for \autoref{ch:information_flow}.


\todo{tikz of how the time synced entropy works}

\todo{we can extend this with predictability}
\subsection{Validating the Assumptions Cross Entropy Estimation}
legth vs cross entropy


% test what suffiently long means in complexity ( plot of complexity vs speed of convegence)


\subsection{Information Flow}\label{ch:information_flow}


%
%	\begin{tikzpicture}
%
%%% TOP EVENTS
%
%\node at (-1, 0.5) {Source};
%
%% Events
%% Past Events
%\draw [thick, fill = blue!30!white] (0,0)  rectangle (10,1)  ;
%\foreach \x in {0, 0.8, 1.6, 3.2, 3.6, 5.2, 6, 8.4, 9.2}
%{
%	\fill[red!30!white] (\x ,0) rectangle ++(0.4,1);
%}
%
%% Future Events
%\draw [thick, fill = blue!60!white] (10,0) rectangle (14,1) ;
%\foreach \x in {10, 10.8, 12.8}
%{
%	\fill[red!60!white] (\x,0) rectangle ++(0.4,1);
%}
%
%% Redraw Main Rectangle
%\foreach \x in {0,...,34}
%{
%	\draw [very thin] (0.4*\x,0)  rectangle ++(0.4,1);
%}
%\draw [thick] (0,0) rectangle (14,1) ;
%
%
%
%%%ORANGE EVENTS
%
%\node at (-1, 0.5) {Source};
%
%% Events
%% Past Events
%\draw [thick, fill = blue!30!white] (0,0)  rectangle (10,1)  ;
%\foreach \x in {0, 0.8, 1.6, 3.2, 3.6, 5.2, 6, 8.4, 9.2}
%{
%	\fill[red!30!white] (\x ,0) rectangle ++(0.4,1);
%}
%
%% Future Events
%\draw [thick, fill = blue!60!white] (10,0) rectangle (14,1) ;
%\foreach \x in {10, 10.8, 12.8}
%{
%	\fill[red!60!white] (\x,0) rectangle ++(0.4,1);
%}
%
%% Redraw Main Rectangle
%\foreach \x in {0,...,34}
%{
%	\draw [very thin] (0.4*\x,0)  rectangle ++(0.4,1);
%}
%\draw [thick] (0,0) rectangle (14,1) ;
%
%
%
%% Future
%\draw [red!60!black, very thick, shorten >= -0.6pt]        (10,-2.5 ) -- (10,1.5);
%\draw [red!60!black, very thick, ->] (10,   1.5)  -- (13, 1.5)   node[midway, above] {Future} ;
%
%% Time Arrow
%\draw [very thick, ->] (0,2.5) -- (13,2.5) node[midway, above] {Time} ;
%
%% Past brace
%\usetikzlibrary{decorations.pathreplacing}
%\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=4pt} ] (9.8,1) -- (0.2,1)
%node[midway, above, yshift = 14pt] {Past} ;
%
%\end{tikzpicture}


%old

%\resizebox{1\textwidth}{!}{
%	\begin{tikzpicture}
%	
%	%% TOP EVENTS
%	
%	\node at (-1, 0.5) {Source};
%	
%	% Events
%	% Past Events
%	\draw [thick, fill = blue!30!white] (0,0)  rectangle (10,1)  ;
%	\foreach \x in {0, 0.8, 1.6, 3.2, 3.6, 5.2, 6, 8.4, 9.2}
%	{
%		\fill[red!30!white] (\x ,0) rectangle ++(0.4,1);
%	}
%	
%	% Future Events
%	\draw [thick, fill = blue!60!white] (10,0) rectangle (14,1) ;
%	\foreach \x in {10, 10.8, 12.8}
%	{
%		\fill[red!60!white] (\x,0) rectangle ++(0.4,1);
%	}
%	
%	% Redraw Main Rectangle
%	\foreach \x in {0,...,34}
%	{
%		\draw [very thin] (0.4*\x,0)  rectangle ++(0.4,1);
%	}
%	\draw [thick] (0,0) rectangle (14,1) ;
%	
%	
%	
%	%%ORANGE EVENTS
%	
%	\node at (-1, 0.5) {Source};
%	
%	% Events
%	% Past Events
%	\draw [thick, fill = blue!30!white] (0,0)  rectangle (10,1)  ;
%	\foreach \x in {0, 0.8, 1.6, 3.2, 3.6, 5.2, 6, 8.4, 9.2}
%	{
%		\fill[red!30!white] (\x ,0) rectangle ++(0.4,1);
%	}
%	
%	% Future Events
%	\draw [thick, fill = blue!60!white] (10,0) rectangle (14,1) ;
%	\foreach \x in {10, 10.8, 12.8}
%	{
%		\fill[red!60!white] (\x,0) rectangle ++(0.4,1);
%	}
%	
%	% Redraw Main Rectangle
%	\foreach \x in {0,...,34}
%	{
%		\draw [very thin] (0.4*\x,0)  rectangle ++(0.4,1);
%	}
%	\draw [thick] (0,0) rectangle (14,1) ;
%	
%	
%	
%	% Future
%	\draw [red!60!black, very thick, shorten >= -0.6pt]        (10,-2.5 ) -- (10,1.5);
%	\draw [red!60!black, very thick, ->] (10,   1.5)  -- (13, 1.5)   node[midway, above] {Future} ;
%	
%	% Time Arrow
%	\draw [very thick, ->] (0,2.5) -- (13,2.5) node[midway, above] {Time} ;
%	
%	% Past brace
%	\usetikzlibrary{decorations.pathreplacing}
%	\draw [very thick, -, draw=black, decorate, decoration={brace,amplitude=10pt,mirror,raise=4pt} ] (9.8,1) -- (0.2,1)
%	node[midway, above, yshift = 14pt] {Past} ;
%	
%	\end{tikzpicture}
%}