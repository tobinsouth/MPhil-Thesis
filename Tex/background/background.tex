%!TEX root = ../thesis.tex
\chapter{Background \label{ch:background}}

\subsection{Natural Language Processing}

Natural language processing (NLP) is the area of study in which 'natural' human language is examined via machine. Natural language refers to either spoken or written language, designed to be understandable to a human listener or reader. This language is not explicitly designed to be machine understandable, and machine comprehension of this language is a challenging problem \tocite{cite: the challenges of NLP}.

NLP is a broad term covering many models and techniques to computationally extracting meaningful information from text, ranging from the simple extraction of individual words, to the extraction of deeper semantic meaning. 

Early work in NLP focused around simple grammatical rules and small vocabularies, such as the work of Georgetown-IBM \tocite{cite: John Hutchins. From first conception to first demonstration: the nascent years of machine translation, 1947–1954. a chronology. Machine Translation, 12(3):195–252, 1997.} to translate 60 sentences from Russian to English in 1954. With the rapid increase in computational power and digital text corpuses, modern NLP has focused or deeper challenges of extracting meaning from text with tools such as Word2Vec \tocite{cite: word to vec} or deep learning methods such as Google's BERT \tocite{cite: BERT}.

These methods face a daunting challenge, language is not only complex and often duplicitous, but contextual and ever-changing. \todo{end better}

\subsubsection{Tokenisation} 

\subsection{Information Theory}
\subsubsection{Entropy}
Entropy is a measure of the uncertainty of a random variable. In the context of information theory, this is defined by \autoref{shannon}, often refereed to as Shannon entropy, named after Claude Shannon for his work in 1948 studying the quantiles of information in transmitted messages~\todo{cite Claude shannon 1948}. The definitions hereafter are sourced from Elements of Information Theory by Thomas and Cover \tocite{elements of information theory} 

\begin{definition}[Shannon Entropy]
	Let $X$ be a discrete random variable with alphabet $\mathcal{X}$ and probability mass function $p(x) = P(X = x), x \in \mathcal{X}$.
	The entropy $H(X)$ of the discrete random variable X, measured in bits, is 
	\begin{equation}\label{eq:shannon}
	H(X)=-\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
	\end{equation}
\end{definition} 

The entropy of the random variable is measured in bits. A bit can have two states, typically 0 or 1. The entropy of a random variable is the number of bits on average that is required to describe the random variable in question. To measure the entropy in bits, we use a logarithm of base 2, and all logarithms throughout this work are assumed to be in based 2, unless otherwise specified.

To give a typical example of entropy, if a fair coin is tossed there are two equally probable outcomes, giving an entropy of 1 bit. Further, we use the convention of $0\log 0 = 0$, which sensibly means that adding a state with 0 probably to the random variable does not change it's entropy.

\begin{remark}[Suprise]
	The entropy of the random variable X can also be described in terms of the expected surprise, where the surprise of a state is $log \frac{1}{p(x)}$.
	\begin{equation}
		H(X) = \mathbb{E} \left[ \frac{1}{p(x)} \right]
	\end{equation}
\end{remark}	



\todo{add some filler}


\begin{lemma}
	The entropy of a random variable is strictly non-negative, $H(X) \geq 0$.
\end{lemma}

\begin{proof}
	$0 \leq p(x) \leq 1$ which implies that $log \frac{1}{p(x)} \geq 0$,
	hence the sum of products of strictly non-negative terms will always be non-negative. 
\end{proof}

\todo{add some filler}

\subsubsection{Joint Entropy and Conditional Entropy}

\begin{definition}[Joint Entropy]
	The joint entropy $H(X,Y)$ of a pair of discrete random variables $(X,Y)$ with a joint distribution $p(x,y)$ and state spaces $(\mathcal{X}, \mathcal{Y})$
	\begin{equation}
	H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
	\end{equation}
\end{definition}


\begin{definition}[Conditional Entropy]
	The conditional entropy $H(X|Y)$ of a pair of discrete random variables $(X,Y)$ with a joint distribution $p(x,y)$ is and state spaces, 
		\begin{align}
		H(X | Y)&=\sum_{y \in \mathcal{Y}} p(y) H(X |Y=y) \\
		&=-\sum_{y \in \mathcal{Y}} p(y) \sum_{y \in \mathcal{Y}} p(x | y) \log p(x | y) \\ 
		&=-\sum_{y \in \mathcal{Y}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x | y) \\ 
		&=-E \log p(X | Y)
		\end{align}
\end{definition}

\begin{remark}
	Importantly, note that $H(X|Y) \neq  H(Y|X)$, a property we will exploit later.
\end{remark}

\subsubsection{Distances}
We can extend these ideas to explore the distances between probability distributions. Kullback–Leibler divergence (also called Kullback–Leibler distance) is a measure of the inefficiency if one were to assume that a distribution is $p$ when the true distribution is $q$.

\begin{definition}[Kullback–Leibler divergence]
	Kullback–Leibler divergence (also called relative entropy), $D(p||q)$,  between two probability mass functions $p(x)$ and $q(x)$ is,
	\begin{align} 
		D(p \| q) &=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \\ 
					 &=E_{p} \log \frac{p(X)}{q(X)} 
	\end{align}
	
\end{definition}

\subsubsection{Predictability}
