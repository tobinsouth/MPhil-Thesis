%!TEX root = ../thesis.tex
\chapter{Background \label{ch:background}}

\todo{Introduction to background}
Information theory, what is it, why do we use it
Networks, why
Natural language processing


\subsection{Information Theory}

\subsubsection{Entropy}
Entropy is a measure of the uncertainty of a random variable. In the context of information theory, this is defined by \autoref{eq:shannon}, often refereed to as Shannon entropy, named after Claude Shannon for his work in 1948 studying the quantiles of information in transmitted messages~\todo{cite Claude shannon 1948}. The definitions hereafter are sourced from Elements of Information Theory by Thomas and Cover \tocite{elements of information theory} 

\begin{definition}[Shannon Entropy]
	Let $X$ be a discrete random variable with alphabet $\mathcal{X}$ and probability mass function $p(x) = P(X = x), x \in \mathcal{X}$.
	The entropy $H(X)$ of the discrete random variable X, measured in bits, is 
	\begin{equation}\label{eq:shannon}
	H(X)=-\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
	\end{equation}
\end{definition} 

The entropy of the random variable is measured in bits. A bit can have two states, typically 0 or 1. The entropy of a random variable is the number of bits on average that is required to describe the random variable in question. To measure the entropy in bits, we use a logarithm of base 2, and all logarithms throughout this work are assumed to be in based 2, unless otherwise specified.

To give a typical example of entropy, if a fair coin is tossed there are two equally probable outcomes, giving an entropy of 1 bit. Further, we use the convention of $0\log 0 = 0$, which sensibly means that adding a state with 0 probably to the random variable does not change it's entropy.

\begin{remark}[Suprise]
	The entropy of the random variable X can also be described in terms of the expected surprise, where the surprise of a state is $\log \frac{1}{p(x)}$.
	\begin{equation}
		H(X) = \mathbb{E} \left[ \frac{1}{p(x)} \right]
	\end{equation}
\end{remark}	



\todo{add some filler}


\begin{lemma}
	The entropy of a random variable is strictly non-negative, $H(X) \geq 0$.
\end{lemma}

\begin{proof}
	$0 \leq p(x) \leq 1$ which implies that $log \frac{1}{p(x)} \geq 0$,
	hence the sum of products of strictly non-negative terms will always be non-negative. 
\end{proof}

\todo{add some filler}

\subsubsection{Joint Entropy and Conditional Entropy}

Above we worked with a single random variable. To extend this we introduce a second discrete random variable $Y$. Using this, we extend the one dimensional entropy to joint entropy. 

\begin{definition}[Joint Entropy]
	The joint entropy $H(X,Y)$ of a pair of discrete random variables $(X,Y)$ with a joint distribution $p(x,y)$ and state spaces $(\mathcal{X}, \mathcal{Y})$
	\begin{equation}\label{eq:jointentropy}
	H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
	\end{equation}
\end{definition}

From our definition of entropy and the law of total probability we can create a notion of conditional entropy.

\begin{definition}[Conditional Entropy]
	The conditional entropy $H(X|Y)$ of two discrete random variables $X$ and $Y$ is defined as, 
		\begin{align}
		H(X | Y)&=\sum_{y \in \mathcal{Y}} p(y) H(X |Y=y) \\
		&=-\sum_{y \in \mathcal{Y}} p(y) \sum_{y \in \mathcal{Y}} p(x | y) \log p(x | y) \\ 
		&=-\sum_{y \in \mathcal{Y}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x | y) \\ 
		&=-E \log p(X | Y)
		\end{align}
\end{definition}


Subtly different from the \emph{conditional entropy} is the \emph{cross entropy}. Whereas the \emph{conditional entropy} is the amount of information needed to describe $X$ given the knowledge of $Y$, the \emph{cross entropy} is the amount of information needed to describe $X$ given a optimal coding scheme built from $Y$. 

\begin{definition}[Cross Entropy]\label{def:crossentropy}
	The cross entropy $H_{\times} (q|p)$ between two probability distributions, defined over the same state space, $p$ nd $q$ is defined as, 
	\begin{equation}
	H (q||p)= - \sum_{x} p(x) \log {q(x)}
	\end{equation}
\end{definition}

Although cross entropy has the common notation $H(X, Y)$, in this thesis we will use an alternative $H(X||Y)$, reminiscent if the Kullback–Leibler divergence in~\autoref{eq:kldivergence} below, so as to not confuse the cross entropy with the above join entropy~\autoref{eq:jointentropy} of the same notation.



\begin{remark}
	Importantly, note that $H(X|Y) \neq  H(Y|X)$ and $H(q||p) \neq H (q||p)$, both properties we will exploit later.
\end{remark}


\subsubsection{Distances}
We can extend these ideas to explore a notion of distance between probability distributions. Kullback–Leibler divergence is a measure of the inefficiency if one were to assume that a distribution is $p$ when the true distribution is $q$.

\begin{definition}[Kullback–Leibler divergence]
	The Kullback–Leibler divergence (also called relative entropy), $D(p \|q)$,  between two probability distributions $p(x)$ and $q(x)$ is,
	\begin{align} \label{eq:kldivergence}
		D(p \| q) &=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \\ 
					 &=E_{p} \log \frac{p(X)}{q(X)} 
	\end{align}
\end{definition}

Again, we use the convention that  $0 \log \frac{0}{0} = 0 $ and $p \log \frac{p}{0} = \infty $. 


Conveniently, we can also express the Kullback–Leibler divergence in terms of the cross entropy.
\begin{lemma}
		\begin{equation}
		D(p \| q) = H_p(q) - H(p) 
		\end{equation}
\end{lemma}
\begin{proof}
	\begin{align}
		D(p \| q) &=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \\
		 &= \sum_{x \in \mathcal{X}} p(x) \log p(x)    -     \sum_{x \in \mathcal{X}} p(x) \log q(x)\\
		 &= -H(p)   +    H(q||p)\\
	\end{align}
\end{proof}

The Kullback–Leibler divergence has two difficulties; It's not symmetrical and it can return infinite values. Jensen–Shannon divergence builds from the Kullback–Leibler divergence to solve these problems to a symmetric, finite comparison between probability distributions. 

\begin{definition}[Jensen–Shannon divergence]
 The Jensen–Shannon divergence between two probability distributions $p(x)$ and $q(x)$ is,
	\begin{equation}
	\operatorname{JSD}(p \| q)=\frac{1}{2} D(p \| m)+\frac{1}{2} D(q \| m)
	\end{equation}
	using a mixture of the distributions, 	$m=\frac{1}{2}(p+q)$.
\end{definition}


\begin{remark}
	The square root of the Jensen–Shannon divergence provides a metric, often referred to as Jensen–Shannon distance.
\end{remark}

\ts{define metric?}


\ts{add mutual information}

\ts{add variation of information}

\ts{add diagram}


\subsubsection{Entropy Rates}

Entropy rate of a stochastic process describes the amount of information required to describe the future state of a process, conditioned on the information in the history of the process.

"The entropy rate is almost surely an asymptotic lower bound on the per-symbol description length when the process is losslessly encoded" \tocite{Some asymptotic properties of entropy of a
	stationary ergodic data source with applications to data compression,}

\begin{definition}[Entropy Rate]\label{def:entropyrate}
	Let  $\mathcal{X}= \{ X_i \}$ be a stochastic ergodic process with a finite alphabet, where $X_i^j$ denotes a subsequence of the process $(X_{i}, X_{i+1}, \ldots, X_{j})$.
	The entropy rate can be defined as,
	\begin{equation}\label{eq:entropyrate}
	H(\mathcal{X})=\lim _{n \rightarrow \infty} H\left(X_{n} | X_{n-1}, X_{n-2}, \ldots, X_{1}\right)
	\end{equation}
	Which, on the assumption of stationary, can be expressed as,
	\begin{equation}
	H(\mathcal{X})=\lim _{n \rightarrow \infty} \frac{1}{n} H\left(X_{1}, X_{2}, \ldots, X_{n}\right)
	\end{equation}
\end{definition}

While this notion of entropy rate provides a valuable theoretical tool, calculating it for real examples can prove difficult, and often impossible given data. In \autoref{ch:crossentropy} we will explore a method of estimating a similar quantity.



\subsubsection{Predictability}

Predictability is the probability $\pi$ that an theoretical predictive algorithm could predict the next state of a process correctly, this often be difficult to obtain. However, an upper bound, $\pi \leq \pi^{max}(S,N)$, is possible through the use of Fano's inequality~\cite{fano_transmission_1961}. For a process with $\pi^{max} = 0.3$, at best we could hope to predict this process correctly 30\% of the time, no matter how good our predicative algorithm~\cite{song_limits_2010}.


\begin{definition}[Maximal Predictability]
For a process $X$ with entropy $H(X)$, Fano's inequality in the context of our maximal predictability gives,
\begin{equation}
H(X) = H(\pi^{max}) + (1 - \pi^{max}) \log (|\mathcal{X}| - 1)
\end{equation}
\end{definition}

The entropy of the maximal predictability $H(\pi^{max})$ is substituted with the binary entropy function~\cite{song_limits_2010},   
\begin{equation}
H(\pi^{max}) = -\pi^{max} \log(\pi^{max}) - (1 -  \pi^{max}) \log(1 - \pi^{max}).
\end{equation}

Which finally gives us a form that can be solved numerically for the fundamental limit of the process' predictability, $\pi^{max}$,
\begin{equation}\label{eq:predict}
-H(X)  = \pi^{max} \log(\pi^{max}) + (1 -  \pi^{max}) \log(1 - \pi^{max}) - (1 - \pi^{max}) \log (|\mathcal{X}| - 1).
\end{equation}


Throughout this thesis, maximal predictabilities will found by solving \autoref{eq:predict} using the Powell's conjugate direction method, implemented in python using SciPy~\cite{virtanen_scipy_2019}, with a starting estimate for the root at $\pi^{max}=0.5$.



% cross predictaibility
We extend this notion of maximal predictability of a process, to create a cross predictability using cross entropy~\autoref{def:crossentropy}.
\todo{more}



\subsection{Networks}

\todo{go through Newman networks and state a bunch of definitions}









\subsection{Natural Language Processing}

\ts{introduce in here the notion that we're interested in text and NLP is a fundamental building block of understanding it}

Natural language processing (NLP) is the area of study in which 'natural' human language is examined via machine. Natural language refers to either spoken or written language, designed to be understandable to a human listener or reader. This language is not explicitly designed to be machine understandable, and machine comprehension of this language is a challenging problem \tocite{cite: the challenges of NLP}.

NLP is a broad term covering many models and techniques to computationally extracting meaningful information from text, ranging from the simple extraction of individual words, to the extraction of deeper semantic meaning. 

Early work in NLP focused around simple grammatical rules and small vocabularies, such as the work of Georgetown-IBM \tocite{cite: John Hutchins. From first conception to first demonstration: the nascent years of machine translation, 1947–1954. a chronology. Machine Translation, 12(3):195–252, 1997.} to translate 60 sentences from Russian to English in 1954. With the rapid increase in computational power and digital text corpuses, modern NLP has focused or deeper challenges of extracting meaning from text with tools such as Word2Vec \tocite{cite: word to vec} or deep learning methods such as Google's BERT \tocite{cite: BERT}.

These methods face a daunting challenge, language is not only complex and often duplicitous, but contextual and ever-changing. \todo{end better}

\subsubsection{Tokenisation} 




